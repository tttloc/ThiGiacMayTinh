{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2023-04-19T12:13:58.559463Z","iopub.execute_input":"2023-04-19T12:13:58.559919Z","iopub.status.idle":"2023-04-19T12:14:02.797530Z","shell.execute_reply.started":"2023-04-19T12:13:58.559825Z","shell.execute_reply":"2023-04-19T12:14:02.796308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import thư viện","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tensorflow.keras.utils import to_categorical\nimport itertools\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:02.799413Z","iopub.execute_input":"2023-04-19T12:14:02.799848Z","iopub.status.idle":"2023-04-19T12:14:13.340823Z","shell.execute_reply.started":"2023-04-19T12:14:02.799806Z","shell.execute_reply":"2023-04-19T12:14:13.339777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Label File","metadata":{}},{"cell_type":"code","source":"label_data = pd.read_csv(\"../input/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/classInd.txt\", sep=' ', header=None)\nlabel_data.columns=['index', 'labels']\nlabel_data = label_data.drop(['index'], axis=1)\nlabel_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.341917Z","iopub.execute_input":"2023-04-19T12:14:13.342748Z","iopub.status.idle":"2023-04-19T12:14:13.388289Z","shell.execute_reply.started":"2023-04-19T12:14:13.342714Z","shell.execute_reply":"2023-04-19T12:14:13.386819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tổng số các video\nlen(label_data)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.391091Z","iopub.execute_input":"2023-04-19T12:14:13.391423Z","iopub.status.idle":"2023-04-19T12:14:13.397476Z","shell.execute_reply.started":"2023-04-19T12:14:13.391393Z","shell.execute_reply":"2023-04-19T12:14:13.396050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting đường dẫn từ data","metadata":{}},{"cell_type":"code","source":"path=[]\nfor label in label_data.labels.values:\n    path.append('../input/ucf101/UCF101/UCF-101/'+label+\"/\")\npath[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.398717Z","iopub.execute_input":"2023-04-19T12:14:13.399558Z","iopub.status.idle":"2023-04-19T12:14:13.416417Z","shell.execute_reply.started":"2023-04-19T12:14:13.399515Z","shell.execute_reply":"2023-04-19T12:14:13.415231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preperation","metadata":{}},{"cell_type":"code","source":"#Function for Feature Extraction\ndef feature_extraction(video_path):\n    width=60\n    height=60\n    sequence_length=10\n    frames_list=[]\n    #Read the Video\n    video_reader = cv2.VideoCapture(video_path)\n    #get the frame count\n    frame_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    #Calculate the interval after which frames will be added to the list\n    skip_interval = max(int(frame_count/sequence_length), 1)\n    #iterate through video frames\n    for counter in range(sequence_length):\n        #Set the current frame postion of the video\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, counter * skip_interval)\n        #Read the current frame \n        ret, frame = video_reader.read()\n        if not ret:\n            break;\n        #Resize the image\n        frame=cv2.resize(frame, (height, width))\n        frame = frame/255\n        #Append to the frame\n        frames_list.append(frame)\n    video_reader.release()\n    #Return the Frames List\n    return frames_list","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.417955Z","iopub.execute_input":"2023-04-19T12:14:13.419039Z","iopub.status.idle":"2023-04-19T12:14:13.429644Z","shell.execute_reply.started":"2023-04-19T12:14:13.418992Z","shell.execute_reply":"2023-04-19T12:14:13.428458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below function is used for extracting features from a video file. It takes in the path of the video file as input and returns a list of frames from the video.\n\nThe function first reads the video using the cv2.VideoCapture method. It then calculates the total number of frames in the video using the cv2.CAP_PROP_FRAME_COUNT property. Next, it calculates the interval after which frames will be added to the list by dividing the total number of frames by the desired sequence length. This is done to ensure that we only extract a certain number of frames from the video.\n\nThe function then iterates through the video frames and sets the current frame position using the cv2.CAP_PROP_POS_FRAMES property. It then reads the current frame and resizes it to a desired width and height using the cv2.resize method. The frame is then normalized by dividing it by 255. Finally, the frame is appended to the frames_list.\n\nOnce all the desired frames have been extracted, the function releases the video reader and returns the frames_list.\n\nFor example, if we pass a video file with 100 frames and a sequence length of 10 to this function, it will extract frames at intervals of 10 (0th, 10th, 20th, ... 90th frame) and return a list of 10 frames.\n\n\nWhy we need sequence length?\nSequence length is the number of frames that we want to extract from a video file. It is an important parameter in feature extraction as it determines the number of frames that will be used to represent the video.\n\nThere are several reasons why we might want to specify a particular sequence length:\n\nMemory Constraints: Extracting all the frames from a video file can be computationally expensive and may not be feasible if we have limited memory or processing power. By specifying a sequence length, we can reduce the number of frames that need to be processed and stored in memory, making the feature extraction process more efficient.\n\nLength of Video: Some videos may be very long and contain thousands of frames. In such cases, it may not be necessary or practical to extract all the frames for feature extraction. By specifying a sequence length, we can reduce the number of frames that need to be processed, making the feature extraction process more efficient.\n\nRelevance of Frames: Not all frames in a video may be relevant for feature extraction. By specifying a sequence length, we can ensure that only the most relevant frames are extracted, which can improve the quality of the features.\n\nOverall, sequence length is an important parameter in feature extraction as it helps us to balance the trade-off between the amount of information extracted from the video and the computational resources required to process it.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#Function for loading video files, Process and store in a data set\ndef load_video(datasets):\n    global image\n    label_index=0\n    labels=[]\n    images=[]\n    #Iterate through each foler corresponding to category\n    for folder in datasets:\n        for file in tqdm(os.listdir(folder)):\n            #Get the path name for each video\n            video_path = os.path.join(folder, file)\n            #Extract the frames of the current video\n            frames_list = feature_extraction(video_path)\n            images.append(frames_list)\n            labels.append(label_index)\n        label_index+=1\n    return np.array(images, dtype='float16'), np.array(labels, dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.431440Z","iopub.execute_input":"2023-04-19T12:14:13.432158Z","iopub.status.idle":"2023-04-19T12:14:13.450531Z","shell.execute_reply.started":"2023-04-19T12:14:13.432122Z","shell.execute_reply":"2023-04-19T12:14:13.449323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above function is used to load and process video files and store them in a dataset. It takes in a list of datasets (folders containing video files) as input and returns a tuple of images (a numpy array of extracted frames) and labels (a numpy array of label indices).\n\nThe function first initializes two empty lists: labels and images. It then iterates through each folder in the datasets list, which corresponds to a particular category of videos. For each folder, it iterates through all the video files and gets the path name of each video using the os.path.join method.\n\nIt then calls the feature_extraction function to extract the frames of the current video and stores the resulting list in the images list. It also appends the label index (which corresponds to the category of the video) to the labels list.\n\nOnce all the videos have been processed, the function returns the images and labels as a tuple.\n\nFor example, if we pass a list of two datasets (folders) containing 10 video files each to this function, it will extract the frames from all 20 videos, store them in the images list, and append the corresponding label indices (0 or 1) to the labels list. It will then return a tuple of numpy arrays containing the extracted frames and the label indices.","metadata":{}},{"cell_type":"code","source":"#Use the first 10 video classes for training the model for demonstration.\nimages, labels = load_video(path[:9])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:14:13.452303Z","iopub.execute_input":"2023-04-19T12:14:13.452808Z","iopub.status.idle":"2023-04-19T12:17:35.519192Z","shell.execute_reply.started":"2023-04-19T12:14:13.452759Z","shell.execute_reply":"2023-04-19T12:17:35.518169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shapes\nimages.shape, pd.Series(labels).shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:17:35.520612Z","iopub.execute_input":"2023-04-19T12:17:35.520948Z","iopub.status.idle":"2023-04-19T12:17:35.528965Z","shell.execute_reply.started":"2023-04-19T12:17:35.520918Z","shell.execute_reply":"2023-04-19T12:17:35.527699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train Test Split\nx_train, x_test, y_train, y_test=train_test_split(images, labels, test_size=0.06, random_state=10)\nx_train.shape, x_test.shape, np.array(y_train).shape, np.array(y_test).shape","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:17:35.532847Z","iopub.execute_input":"2023-04-19T12:17:35.533271Z","iopub.status.idle":"2023-04-19T12:17:35.642647Z","shell.execute_reply.started":"2023-04-19T12:17:35.533239Z","shell.execute_reply":"2023-04-19T12:17:35.641461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mô hình: CNN + LSTM (ConvLSTM2D)","metadata":{}},{"cell_type":"markdown","source":"![](https://i.stack.imgur.com/iYmhu.png)\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(BatchNormalization(momentum=0.8, input_shape=(x_train.shape[1],x_train.shape[2], x_train.shape[3], 3)))\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.3)))\n\nmodel.add(ConvLSTM2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last', return_sequences=True, recurrent_dropout=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.3)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(4096,activation=\"relu\"))\n    \nmodel.add(Dense(9, activation='softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:17:35.645014Z","iopub.execute_input":"2023-04-19T12:17:35.645789Z","iopub.status.idle":"2023-04-19T12:17:36.374674Z","shell.execute_reply.started":"2023-04-19T12:17:35.645742Z","shell.execute_reply":"2023-04-19T12:17:36.373454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"mô hình Convolutional Long Short-Term Memory (ConvLSTM) cho phân loại video. Bao gồm một số lớp ConvLSTM2D, MaxPooling3D, Dropout, BatchNormalization và Dense.\n\nĐầu vào của mô hình là một dãy các khung hình video có dạng (batch_size, sequence_length, height, width, channels). Lớp đầu tiên là một lớp BatchNormalization, nó chuẩn hóa dữ liệu đầu vào sử dụng trung bình và độ lệch chuẩn của batch\n\nTiếp theo là lớp ConvLSTM2D, là một loại lớp mạng neural convolutional (CNN) có ô nhớ để xử lý dữ liệu tuần tự. Các lớp này áp dụng bộ lọc convolutional cho dữ liệu đầu vào và sử dụng thuật toán LSTM để xử lý thông tin tuần tự. Tham số kernel_size xác định kích thước của bộ lọc convolutional, và tham số activation xác định hàm kích hoạt được sử dụng. Tham số return_sequences xác định xem kết quả của lớp có được trả về như một dãy hay là một kết quả đầu ra duy nhất. Tham số recurrent_dropout xác định tỷ lệ dropout cho các kết nối tuần tự trong lớp.\n\nCác lớp MaxPooling3D được sử dụng để giảm mẫu dữ liệu đầu vào bằng cách lấy giá trị lớn nhất qua một cửa sổ giảm mẫu của kích thước đã chỉ định. Tham số padding xác định xem dữ liệu đầu vào có được lấp đầy trước khi áp dụng thao tác giảm mẫu.\n\nCác lớp TimeDistributed được sử dụng để áp dụng một lớp cho mỗi bước của một dãy độc lập. Các lớp Dropout được sử dụng để đặt ngẫu nhiên một phần của đơn vị đầu vào thành 0 tại mỗi lần cập nhật trong quá trình huấn luyện, điều này giúp ngăn ngừa overfitting.\n\nCác lớp BatchNormalization được sử dụng để chuẩn hóa các hoạt động của các lớp trước sử dụng trung bình và độ lệch chuẩn của batch.\n\nCuối cùng, mô hình có một lớp Flatten để làm phẳng kết quả của lớp trước, theo sau là hai lớp Dense. Lớp dense đầu tiên có 4096 đơn vị và sử dụng hàm kích hoạt ReLU. Lớp dense thứ hai có 9 đơn vị và sử dụng hàm kích hoạt softmax, được thích hợp cho các tác vụ phân loại. Hàm softmax sẽ đưa ra một phân bố xác suất qua 9 lớp có thể, cho phép mô hình dự đoán lớp có xác suất cao nhất.","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:17:36.376236Z","iopub.execute_input":"2023-04-19T12:17:36.376680Z","iopub.status.idle":"2023-04-19T12:17:36.393823Z","shell.execute_reply.started":"2023-04-19T12:17:36.376636Z","shell.execute_reply":"2023-04-19T12:17:36.392591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train model\nes = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\nhistory = model.fit(x_train, to_categorical(y_train), batch_size=32, epochs=50, validation_data=(x_test, to_categorical(y_test)), callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:17:36.395703Z","iopub.execute_input":"2023-04-19T12:17:36.396599Z","iopub.status.idle":"2023-04-19T13:01:49.220828Z","shell.execute_reply.started":"2023-04-19T12:17:36.396564Z","shell.execute_reply":"2023-04-19T13:01:49.219542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Xác định một hàm callback EarlyStopping cho quá trình huấn luyện mô hình. Hàm callback EarlyStopping giám sát mất mát xác nhận và dừng quá trình huấn luyện khi mất mát xác nhận không cải thiện trong số vài epoch.\n\nTham số monitor xác định số lượng cần giám sát. Trong trường hợp này, đó là mất mát xác nhận. Tham số patience xác định số epoch để chờ trước khi dừng quá trình huấn luyện nếu mất mát xác nhận không cải thiện. Tham số mode xác định số lượng được giám sát có nên được tối thiểu hoặc tối đa. Tham số restore_best_weights xác định trọng lượng của mô hình có nên được khôi phục lại trọng lượng tốt nhất (mất mát xác nhận thấp nhất) sau khi quá trình huấn luyện đã dừng.\n\nVí dụ, nếu chúng ta sử dụng hàm callback EarlyStopping này trong quá trình huấn luyện một mô hình, nó sẽ giám sát mất mát xác nhận tại mỗi epoch và dừng quá trình huấn luyện nếu mất mát xác nhận không cải thiện trong 5 epoch. Nếu tham số restore_best_weights được đặt là True, nó cũng sẽ khôi phục trọng lượng của mô hình về trọng lượng tốt nhất (mất mát xác nhận thấp nhất) sau khi quá trình huấn luyện đã dừng.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13,5))\nplt.title(\"Accuracy vs Epochs\")\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Test Accuracy')\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:01:49.222541Z","iopub.execute_input":"2023-04-19T13:01:49.223616Z","iopub.status.idle":"2023-04-19T13:02:16.147639Z","shell.execute_reply.started":"2023-04-19T13:01:49.223567Z","shell.execute_reply":"2023-04-19T13:02:16.146270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\npredicted_classes=[]\nfor i in range(len(y_test)):\n    predicted_classes.append(np.argmax(y_pred[i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:16.149872Z","iopub.execute_input":"2023-04-19T13:02:16.150472Z","iopub.status.idle":"2023-04-19T13:02:18.811123Z","shell.execute_reply.started":"2023-04-19T13:02:16.150422Z","shell.execute_reply":"2023-04-19T13:02:18.810167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, predicted_classes)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:18.812411Z","iopub.execute_input":"2023-04-19T13:02:18.812923Z","iopub.status.idle":"2023-04-19T13:02:18.819689Z","shell.execute_reply.started":"2023-04-19T13:02:18.812893Z","shell.execute_reply":"2023-04-19T13:02:18.818866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion Matrix\nplt.figure(figsize=(25,25))\nplt.title(\"Confusion matrix\")\ncm=confusion_matrix(y_test, predicted_classes)\nplt.imshow(cm)\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment=\"center\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:18.821114Z","iopub.execute_input":"2023-04-19T13:02:18.821684Z","iopub.status.idle":"2023-04-19T13:02:19.581643Z","shell.execute_reply.started":"2023-04-19T13:02:18.821651Z","shell.execute_reply":"2023-04-19T13:02:19.580435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('CNN_LSTM_Model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:14:16.472952Z","iopub.execute_input":"2023-04-19T13:14:16.473498Z","iopub.status.idle":"2023-04-19T13:14:16.597079Z","shell.execute_reply.started":"2023-04-19T13:14:16.473461Z","shell.execute_reply":"2023-04-19T13:14:16.595451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vẽ ma trận nhầm lẫn cho phân loại. Nó nhận vào nhãn thật (y_test) và nhãn dự đoán (predicted_classes) làm đầu vào và vẽ ma trận có kích thước (số lớp) x (số lớp).\n\nMa trận nhầm lẫn là một ma trận cho thấy số lượng các dự đoán đúng và sai cho mỗi lớp. Các hàng của ma trận đại diện cho nhãn thật, và các cột đại diện cho nhãn dự đoán. Một ô (i, j) trong ma trận đại diện cho số lượng trường hợp với nhãn thật i và nhãn dự đoán j.\n\nSử dụng hàm confusion_matrix từ thư viện sklearn để tính ma trận nhầm lẫn. \n\nSau đó, sử dụng hàm imshow từ matplotlib để vẽ ma trận nhầm lẫn dưới dạng hình ảnh. \n\nCuối cùng, sử dụng hàm itertools.product để lặp qua các phần tử của ma trận và thêm giá trị tương ứng vào mỗi ô bằng hàm text.\n\nVí dụ, nếu chúng ta truyền một danh sách nhãn thật [0, 1, 0, 2, 1] và một danh sách nhãn dự đoán [0, 1, 0, 2, 2] cho mã này, nó sẽ vẽ:\n\n![image.png](attachment:fdd5279a-676a-4b98-99a0-c1266d2c694f.png)\n","metadata":{},"attachments":{"fdd5279a-676a-4b98-99a0-c1266d2c694f.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"## Mô hình: CNN (Conv2D)","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(BatchNormalization(momentum=0.8, input_shape=(x_train.shape[1],x_train.shape[2], x_train.shape[3], 3)))\nmodel.add(Conv2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last'))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(Conv2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last'))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.2)))\n\nmodel.add(Conv2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last'))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.3)))\n\nmodel.add(Conv2D(filters = 16, kernel_size=(3,3), activation='LeakyReLU', data_format='channels_last'))\nmodel.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format='channels_last'))\nmodel.add(TimeDistributed(Dropout(0.3)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(4096,activation=\"relu\"))\n    \nmodel.add(Dense(9, activation='softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:19.583145Z","iopub.execute_input":"2023-04-19T13:02:19.583512Z","iopub.status.idle":"2023-04-19T13:02:20.104207Z","shell.execute_reply.started":"2023-04-19T13:02:19.583478Z","shell.execute_reply":"2023-04-19T13:02:20.102963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Một số lớp Conv2D, MaxPooling3D, Dropout và Dense.\n\nĐầu vào của mô hình là một chuỗi các khung hình video có hình dạng (batch_size, sequence_length, height, width, channels). \nTầng đầu tiên BatchNormalization, lớp này chuẩn hóa dữ liệu đầu vào bằng cách sử dụng giá trị trung bình và độ lệch chuẩn.\n\nNhiều lớp Conv2D, đó là một loại lớp CNN áp dụng bộ lọc tích chập cho dữ liệu đầu vào. Tham số kernel_size xác định kích thước của bộ lọc tích chập, và tham số activation xác định hàm kích hoạt sẽ được sử dụng.\n\nLớp MaxPooling3D được sử dụng để giảm mẫu dữ liệu đầu vào bằng cách lấy giá trị lớn nhất trên cửa sổ lấy mẫu của kích thước đã chỉ định. Tham số padding xác định liệu dữ liệu đầu vào có được lấp chừa trước khi áp dụng hoạt động lấy mẫu.\n\nLớp TimeDistributed được sử dụng để áp dụng một lớp cho mỗi bước của một dãy độc lập. Lớp Dropout được sử dụng để đặt ngẫu nhiên một phần của đơn vị đầu vào thành 0 tại mỗi lần cập nhật trong quá trình huấn luyện, điều này giúp ngăn chặn quá huấn luyện.\n\nCuối cùng, mô hình có một lớp Flatten để phẳng hóa đầu ra của lớp trước, sau đó là hai lớp Dense. Lớp dense đầu tiên có 4096 đơn vị và sử dụng hàm kích hoạt ReLU. Lớp dense thứ hai có 9 đơn vị và sử dụng hàm kích hoạt softmax, đó là phù hợp cho nhiệm vụ phân loại. Tóm tắt mô hình sau đó được in ra để hiển thị chi tiết của mô hình.","metadata":{}},{"cell_type":"code","source":"#compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:20.105530Z","iopub.execute_input":"2023-04-19T13:02:20.105940Z","iopub.status.idle":"2023-04-19T13:02:20.117776Z","shell.execute_reply.started":"2023-04-19T13:02:20.105910Z","shell.execute_reply":"2023-04-19T13:02:20.116298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model training\nes = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\nhistory = model.fit(x_train, to_categorical(y_train), batch_size=64, epochs=50, validation_data=(x_test, to_categorical(y_test)), callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:02:20.119028Z","iopub.execute_input":"2023-04-19T13:02:20.119713Z","iopub.status.idle":"2023-04-19T13:05:52.547723Z","shell.execute_reply.started":"2023-04-19T13:02:20.119680Z","shell.execute_reply":"2023-04-19T13:05:52.546535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vẽ biểu đồ để kiểm tra độ chính xác của tập train và tập test trong khoảng thời gian\nplt.figure(figsize=(13,5))\nplt.title(\"Accuracy vs Epochs\")\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Test Accuracy')\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:14:34.535712Z","iopub.execute_input":"2023-04-19T13:14:34.536272Z","iopub.status.idle":"2023-04-19T13:14:34.809441Z","shell.execute_reply.started":"2023-04-19T13:14:34.536225Z","shell.execute_reply":"2023-04-19T13:14:34.807857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\npredicted_classes=[]\nfor i in range(len(y_test)):\n    predicted_classes.append(np.argmax(y_pred[i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:14:38.889163Z","iopub.execute_input":"2023-04-19T13:14:38.889627Z","iopub.status.idle":"2023-04-19T13:14:39.298156Z","shell.execute_reply.started":"2023-04-19T13:14:38.889588Z","shell.execute_reply":"2023-04-19T13:14:39.297097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test Accuracy\naccuracy_score(y_test, predicted_classes)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:14:43.242701Z","iopub.execute_input":"2023-04-19T13:14:43.243153Z","iopub.status.idle":"2023-04-19T13:14:43.252662Z","shell.execute_reply.started":"2023-04-19T13:14:43.243116Z","shell.execute_reply":"2023-04-19T13:14:43.251524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('CNN_Model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:15:03.715056Z","iopub.execute_input":"2023-04-19T13:15:03.715499Z","iopub.status.idle":"2023-04-19T13:15:03.817038Z","shell.execute_reply.started":"2023-04-19T13:15:03.715463Z","shell.execute_reply":"2023-04-19T13:15:03.815667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, predicted_classes)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:05:53.363160Z","iopub.execute_input":"2023-04-19T13:05:53.363465Z","iopub.status.idle":"2023-04-19T13:05:53.374759Z","shell.execute_reply.started":"2023-04-19T13:05:53.363437Z","shell.execute_reply":"2023-04-19T13:05:53.374034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:05:53.376034Z","iopub.execute_input":"2023-04-19T13:05:53.376624Z","iopub.status.idle":"2023-04-19T13:05:53.391658Z","shell.execute_reply.started":"2023-04-19T13:05:53.376595Z","shell.execute_reply":"2023-04-19T13:05:53.390897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}